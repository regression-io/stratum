# Implementation Path

## TypeScript Is Not the Only Option

The LLM developer community is split across two ecosystems that barely overlap:

| Ecosystem | Language | Who's there |
|---|---|---|
| AI/ML practitioners | Python | DSPy, LangChain, instructor, Pydantic, AutoGen, researchers |
| Fullstack/production | TypeScript | Vercel AI SDK, LangChain.js, Mastra, startups |
| High-performance runtime | Rust | Infrastructure, model serving, compiler tooling |

Picking TypeScript captures one half and misses the other entirely. The Python crowd is where the pain is most acute right now — they're the ones building multi-agent systems today with the most ad hoc glue code.

---

## Python First

Python already has the substrate. The Stratum patterns map directly to idiomatic Python:

```python
@contract
class SentimentResult(BaseModel):
    label: Literal["positive", "negative", "neutral"]
    confidence: float = Field(ge=0.0, le=1.0)
    reasoning: str

@infer(
    intent="Classify emotional tone of customer feedback",
    context="Sarcasm → negative. Ambiguous → neutral.",
    ensure=lambda r: r.confidence > 0.7,
    budget=Budget(ms=500, usd=0.001)
)
def classify_sentiment(text: str) -> SentimentResult: ...

@flow
def process_ticket(ticket: SupportTicket) -> Resolution:
    category  = classify(ticket.body)
    sentiment = classify_sentiment(ticket.body)
    response  = draft_response(ticket, category, sentiment)
    approved  = rule_check(response, category)
    return approved or escalate(ticket)
```

- `contract` → plain annotated class or Pydantic `BaseModel` (optional enhanced backend)
- `infer` → decorator wrapping typed LLM call with `ensure` validation and structured retry
- `flow` → typed composition with sequential type checking
- `compute` → plain Python function, marked explicitly

**Migration path from existing code is short.** DSPy and instructor users recognize the pattern immediately.

---

## TypeScript Second

Same design, idiomatic TypeScript surface:

```typescript
const SentimentResult = contract(z.object({
  label: z.enum(["positive", "negative", "neutral"]),
  confidence: z.number().min(0).max(1),
  reasoning: z.string()
}))

const classifySentiment = infer({
  schema: SentimentResult,
  intent: "Classify emotional tone of customer feedback",
  context: "Sarcasm → negative. Ambiguous → neutral.",
  ensure: (r) => r.confidence > 0.7,
  budget: { ms: 500, usd: 0.001 }
})
```

- `contract` → Zod schema + derived TypeScript types
- `infer` → typed wrapper with prompt compilation, `ensure`, structured retry, budget
- `flow` → typed composition utility

**Dependencies (v1):**
- `zod` — contracts
- `@anthropic-ai/sdk` — LLM calls (direct, minimal, stable)
- Multi-provider path: LiteLLM's OpenAI-compatible endpoint + `openai` TypeScript SDK

**Vercel AI SDK** — integration target for Next.js users who are already in that ecosystem. Not the substrate — it's Vercel's product, optimized for streaming UI patterns and the Vercel platform. Overhead for backend/standalone deployments.

**Target audience:** fullstack teams building LLM features into TypeScript backends, Node.js services, and Next.js apps (via Vercel AI SDK adapter).

---

## Rust Core — When the Compiler Is Worth Building

If/when the prompt compiler and static analysis are worth building properly, Rust is the right substrate:

- Correct by construction — memory safety, no GC pauses in the runtime
- WASM compilation → runs in browsers, embeds anywhere
- PyO3 → idiomatic Python bindings from Rust
- Bun/Node native modules → TypeScript bindings from Rust
- One correct implementation; multiple idiomatic surfaces

**This is the pattern modern tools use:**
- Pydantic v2 core → Rust, Python bindings via PyO3
- Tokenizers → Rust, Python + JS bindings
- Ruff → Rust, replaces Python linting tooling
- Biome → Rust, replaces JS/TS formatting tooling

Build the Stratum runtime in Rust. Generate idiomatic bindings for Python and TypeScript. The core is correct once; the surface is idiomatic everywhere.

---

## The Language-Agnostic IR

The `.stratum.yaml` format is not a source language developers author — it's the IR that the Python and TypeScript libraries compile to internally, and that Claude emits when generating execution plans. Like LLVM IR or bytecode: tools target it, humans read it for debugging, nobody writes it from scratch.

```yaml
# generated by @infer decorator or Claude — not hand-authored
functions:
  classifySentiment:
    mode: infer
    intent: "Classify emotional tone of customer feedback"
    context: "Sarcasm → negative. Ambiguous → neutral."
    input:
      text: string
    output: SentimentResult
    ensure:
      - result.confidence > 0.7
    budget:
      ms: 500
      usd: 0.001
```

Any language runtime can execute a `.stratum` spec. This is the interchange format between authoring surfaces (Python, TypeScript, natural language) and execution backends (MCP server, direct runtime, Agent SDK). The spec is the stable contract between them; the libraries are the on-ramp developers actually touch.

---

## The Full Sequence

```
Phase 1 — Python library (minimal, prove the ideas)
  One required dependency: litellm
  Optional: pydantic (enhanced @contract), otel (trace export)
  Python 3.11+ stdlib for everything else (asyncio.TaskGroup, asyncio.timeout)

  - @contract — plain annotated class, Pydantic as optional enhanced backend
  - @infer — async, ensure + structured retry, asyncio.timeout for budget
  - @compute — deterministic function marker
  - @flow — typed composition, asyncio.TaskGroup for parallel
  - internal trace records — always written in-memory
  - OTel export — optional extra
  - stratum.run() — sync shim for scripts and notebooks

  Open source, gather feedback.

Phase 2 — MCP server + Claude Code wiring + TypeScript library
  MCP server
    Additional dependencies: jsonschema, pyyaml (IR validation/parsing), mcp SDK
    Wraps the Phase 1 library. Adds stratum_validate, stratum_execute, stratum_plan tools.

  TypeScript library
    - @contract — Zod schemas (z.object, z.enum, z.number constraints)
    - @infer — wraps Anthropic TypeScript SDK for v1 LLM calls
    - @flow — typed composition
    - Multi-provider path: LiteLLM's OpenAI-compatible endpoint + OpenAI TypeScript SDK
    - Vercel AI SDK — integration target for Next.js users, not the substrate
    - Same minimal-dependencies principle as Python v1

Phase 3 — Enterprise integrations (build from observed pain, not schedule)
  Temporal  — durable execution for long-running/high-value flows
              Stratum @infer calls wrapped as Temporal activities
              Temporal handles infrastructure retry; Stratum handles semantic retry
              Trigger: flows longer than minutes, or partial execution is expensive

  Ray       — distributed agent execution at scale
              @agent backed by Ray actors for cross-machine distribution
              Trigger: in-process asyncio concurrency hits real limits (100+ parallel branches)

  Outlines  — constrained decoding for self-hosted models
              Integration path: LiteLLM → vLLM → Outlines
              Trigger: teams moving to self-hosted inference

  DSPy      — learned prompt optimization
              compile_prompt() optionally DSPy-backed
              Trigger: teams with labeled data and evaluation sets

---

Phase 4 — Build the compiler (years, if warranted)
  Rust core
    - Parser for Stratum syntax
    - Type checker
    - Prompt compiler
    - Constrained decoding grammar generator
    - Runtime executor
  PyO3 → Python bindings
  WASM/native → TypeScript bindings
  Static analysis tooling
  IDE integration (LSP)
```

---

## What to Build First

The token audit tool is the fastest proof of value — it analyzes existing code, requires no adoption of anything new, and has clear ROI. It's also the best way to validate the core claim: that structural constraints in prompts are a measurable waste.

After that: the Python `infer` decorator with `ensure` and structured retry. This is the highest-leverage single primitive — it replaces hundreds of lines of glue code that every LLM Python project currently writes by hand.

Those two things together are a credible v0.1.
