# How to Build the Claude Code Integration

This is a separate deployment context from the Python library. The audience here is not a developer using `@infer` in their own code — it's an operator configuring Claude Code to use Stratum as its execution runtime.

For the Python library (the primary product), see [`../library/how-to-build.md`](../library/how-to-build.md).

---

## The Division of Responsibility

**Claude Code generates. The MCP server enforces.**

Claude is good at generating structured output. The MCP server validates, executes, and constrains what Claude generates. These are different jobs.

```
Claude Code             MCP Server
────────────            ──────────
Generates plan    →     Validates plan against spec schema
Writes code       →     Validates .stratum artifacts on write (via hook)
Executes flow     →     Enforces contracts, budget, structured retry
Reviews output    →     Audits token efficiency
```

The enforcement problem here is real: nothing stops Claude Code from ignoring the MCP server and writing raw LLM calls. The Python library user doesn't have this problem — they opted in by using `@infer`. Claude Code is an AI agent that can route around constraints. See [`enforcement-gap.md`](enforcement-gap.md) for the full analysis.

---

## Build Order

```
1. Python library          — the execution engine the MCP server wraps
2. .stratum IR spec        — the interchange format between Claude and the runtime
3. MCP server              — exposes library capabilities as Claude Code tools
4. Claude Code wiring      — settings, skills, hooks, CLAUDE.md
```

The MCP server wraps the Python library. Build the library first.

---

## 1. The `.stratum` IR Spec

The artifact Claude generates and the MCP server validates. This is an IR — Claude emits it, developers don't write it by hand.

```yaml
# generated by Claude or the Python library — not hand-authored
version: "0.1"

contracts:
  TokenSchema:
    fields:
      type: array
      items: string
    authStrategy:
      type: enum
      values: [jwt, session, oauth]

  Middleware:
    code: string
    handlesExpiry: boolean
    routes:
      type: array
      items: string

functions:
  designTokenSchema:
    mode: infer
    intent: "Design the JWT token schema for this Express API"
    input:
      existingModels:
        type: array
        items: string
    output: TokenSchema
    budget: { ms: 5000, usd: 0.01 }

  writeMiddleware:
    mode: infer
    intent: "Write Express JWT authentication middleware"
    input:
      schema: TokenSchema
    output: Middleware
    ensure:
      - "result.handlesExpiry == true"
    budget: { ms: 10000, usd: 0.02 }
    retries: 3

flows:
  addAuth:
    input:
      existingModels:
        type: array
        items: string
    output: Middleware
    budget: { ms: 30000, usd: 0.05 }
    steps:
      - id: schema
        function: designTokenSchema
        inputs:
          existingModels: "$.input.existingModels"
      - id: middleware
        function: writeMiddleware
        inputs:
          schema: "$.steps.schema.output"
        depends_on: [schema]
```

Build the JSON schema validator for `.stratum.yaml` first. Everything downstream depends on it being correct.

---

## 2. The MCP Server

### Project structure

```
stratum-mcp/
├── pyproject.toml
├── src/stratum_mcp/
│   ├── server.py      # MCP server entry point
│   ├── spec.py        # .stratum spec parser and validator
│   ├── executor.py    # wraps stratum library execute_flow
│   └── auditor.py     # token audit tool (v2)
```

### server.py

```python
from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import Tool, TextContent
from .spec import validate_spec
from .executor import execute_flow
from .auditor import audit_file

server = Server("stratum")

@server.list_tools()
async def list_tools():
    return [
        Tool(
            name="stratum_validate",
            description="Validate a .stratum spec file",
            inputSchema={
                "type": "object",
                "properties": {
                    "spec": {"type": "string", "description": "Path to .stratum.yaml or inline YAML"}
                },
                "required": ["spec"]
            }
        ),
        Tool(
            name="stratum_execute",
            description="Execute a .stratum flow with typed inputs. Enforces contracts, budget, and structured retry.",
            inputSchema={
                "type": "object",
                "properties": {
                    "spec":   {"type": "string"},
                    "flow":   {"type": "string"},
                    "inputs": {"type": "object"}
                },
                "required": ["spec", "flow", "inputs"]
            }
        ),
        Tool(
            name="stratum_plan",
            description="Validate a proposed .stratum flow plan before presenting it to the user.",
            inputSchema={
                "type": "object",
                "properties": {
                    "plan": {"type": "string", "description": "Inline .stratum YAML to validate"}
                },
                "required": ["plan"]
            }
        ),
        Tool(
            name="stratum_audit",
            description="Analyze a file for LLM token waste. Returns structural vs semantic token breakdown.",
            inputSchema={
                "type": "object",
                "properties": {"path": {"type": "string"}},
                "required": ["path"]
            }
        ),
    ]

@server.call_tool()
async def call_tool(name: str, arguments: dict):
    match name:
        case "stratum_validate" | "stratum_plan":
            result = validate_spec(arguments.get("spec") or arguments.get("plan"))
            return [TextContent(type="text", text=result.to_json())]
        case "stratum_execute":
            result = await execute_flow(
                spec=arguments["spec"],
                flow=arguments["flow"],
                inputs=arguments["inputs"]
            )
            return [TextContent(type="text", text=result.to_json())]
        case "stratum_audit":
            report = audit_file(arguments["path"])
            return [TextContent(type="text", text=report.to_json())]

async def main():
    async with stdio_server() as (read, write):
        await server.run(read, write, server.create_initialization_options())
```

### Installation

```toml
[project]
name = "stratum-mcp"
version = "0.1.0"
dependencies = [
    "mcp>=0.9",
    "stratum>=0.1",     # the Python library
    "pyyaml>=6.0",
    "jsonschema>=4.0",
]

[project.scripts]
stratum-mcp = "stratum_mcp.server:main"
```

---

## 3. Claude Code Wiring

Four files. Together they create the enforcement stack.

### ~/.claude/settings.json — register the MCP server

```json
{
  "mcpServers": {
    "stratum": {
      "command": "stratum-mcp",
      "args": [],
      "env": {
        "ANTHROPIC_API_KEY": "${ANTHROPIC_API_KEY}"
      }
    }
  }
}
```

### ~/.claude/skills/plan.md — the planning skill

```markdown
Generate and validate a typed execution plan for the current task, then present it
in plain language for user approval.

1. Analyze the task and identify all steps required
2. Determine the output type:
   - TRANSIENT: the result is a response, analysis, or decision — no persistent code written
   - PERSISTENT: the result is code or configuration that lives in the codebase
3. For each step, determine if it is `infer` (requires LLM judgment) or `compute` (deterministic)
4. Identify dependencies between steps
5. Generate a `.stratum.yaml` spec representing the full plan [internal — never shown to user]
6. Call `stratum_plan` with the generated YAML to validate it [internal]
7. If validation fails, fix the issues and revalidate [internal]
8. Present the validated plan to the user in plain language ONLY:
   - A numbered list of what will happen, in plain English
   - Estimated cost (e.g. "~$0.03")
   - Do NOT show step modes, contract names, input/output types, or YAML
   - Example: "1. Analyze the sentiment of each review  2. Draft a response for negative ones
     3. Flag anything below 70% confidence for your review. Estimated cost: ~$0.02."
9. Ask: "Proceed?" — do NOT execute anything until the user explicitly approves
10. On approval, call `stratum_execute` [internal]
11. When writing PERSISTENT output (code files):
    - Only apply Stratum annotations to functions that actually invoke an LLM
    - `@infer` — functions classified as `infer` in step 3 (LLM judgment required)
    - `@compute` — functions classified as `compute` in step 3 (deterministic logic)
    - `@contract` — Pydantic models that define inter-step data shapes
    - `@flow` — the top-level orchestration function
    - Do NOT annotate pure utility functions, I/O helpers, or anything that doesn't touch an LLM
    - Do NOT write raw LLM calls, manual retry loops, or ad hoc prompt construction
    - Import: `from stratum import infer, contract, flow, Budget`
    - The classification from step 3 is the ground truth — the generated code must match it exactly
12. On completion, present a one-line summary: steps completed, actual cost, any retries or
    escalations that occurred in plain language
```

### ~/.claude/settings.json — hooks (add to existing)

```json
{
  "hooks": {
    "PostToolUse": [
      {
        "matcher": "Write",
        "hooks": [{
          "type": "command",
          "command": "bash -c 'if [[ \"$TOOL_INPUT_PATH\" == *.stratum.yaml ]]; then stratum-mcp validate \"$TOOL_INPUT_PATH\" || echo \"STRATUM_VALIDATION_FAILED: fix before proceeding\"; fi'"
        }]
      }
    ]
  }
}
```

### CLAUDE.md — rules for the gaps

```markdown
## Stratum

For any task requiring more than two steps:
1. Call `stratum_plan` with a proposed .stratum flow BEFORE executing anything
2. Present the validated plan for user approval
3. Execute through `stratum_execute`, not as an ad hoc sequence

After writing any LLM orchestration code (Python files with LLM calls):
- Call `stratum_audit` on the file and include the token waste report in your summary

When a step fails:
- Do not retry with the full context
- Identify the specific failure and retry with that failure injected as structured feedback
- This is what the executor does automatically — prefer routing through stratum_execute
```

---

## Build Sequence for This Track

```
Week 2-3:  .stratum IR spec + MCP server
             (after Python library is working)
             - stratum_validate
             - stratum_execute (wraps library executor)
             - stratum_plan

Week 4:    Claude Code wiring
             - settings.json, skills/plan.md, hooks, CLAUDE.md
             - test with real Claude Code sessions before declaring done

Week 5+:   stratum_audit (token efficiency tooling)
             - needs real usage data to produce meaningful findings
```

Do not wire Claude Code before the MCP server is solid. Bad early experiences are worse than a delayed rollout.
