# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Status

Design phase. Nothing is implemented. All files under `docs/` are design notes produced through an extended design session. The implementation has not started.

## What This Project Is

Stratum is a Python (and TypeScript) library where `@infer` and `@compute` functions compose identically, typed contracts flow between steps, and orchestration is always deterministic regardless of what's inside individual steps. The `.stratum.yaml` IR is what the library emits internally — developers never write it.

Two deployment tracks:
- **Track 1 — Python library**: `@infer`, `@contract`, `@flow` decorators. One required dependency (`litellm`). Python 3.11+.
- **Track 2 — Claude Code + MCP**: Stratum as an execution runtime behind Claude Code. Two audiences: professional developers (see typed plans) and vibe coders (see plain-language summaries, get `@infer`-annotated code as output).

## Doc Structure

```
docs/library/       — Python library design (the primary product)
docs/claude-code/   — MCP server + Claude Code integration (Track 2)
docs/strategy/      — competitive analysis, go-to-market, implementation path
```

**Start here for implementation:**
- `docs/library/how-to-build.md` — project structure, executor.py, compiler.py, build sequence
- `docs/library/language-design.md` — semantic model: what `@infer`, `@contract`, `@flow` mean
- `docs/library/execution-model.md` — the full execution loop, async model, LLM client config, OTel

**Key design decisions recorded in:**
- `docs/library/open-problems.md` — all 16 design problems with resolution status
- `docs/library/type-system.md` — contracts, `Probabilistic[T]`, content hash
- `docs/library/concurrency-and-agents.md` — `parallel`, `debate`, isolation model, Ray upgrade path

## Architecture Decisions

**Contracts**: `@contract` works on plain annotated Python classes. Pydantic `BaseModel` is an optional enhanced backend — not required. Stratum generates JSON Schema from `typing.get_type_hints()` internally.

**Async**: runtime is async-first. `@infer` and `@flow` are async natively. Sync shim via `stratum.run()`. Uses `asyncio.TaskGroup` (Python 3.11+) for `parallel`, `asyncio.timeout` for budget enforcement.

**LLM routing**: LiteLLM is the required LLM client substrate — handles multi-model routing, fallback, cost tracking. The `model:` annotation is a hint passed through to LiteLLM.

**Observability**: internal trace records always written in-memory. OTLP export via a built-in emitter (`stratum/exporters/otlp.py`) — HTTP/JSON POST to any OTLP endpoint. No OTel SDK dependency.

**Non-determinism**: `stable=True` (default) → return type is `T`. `stable=False` → return type is `Probabilistic[T]`, caller must unwrap via `.most_likely()`, `.sample()`, or `.assert_stable()`.

**Prompt optimization**: v1 uses a deterministic prompt compiler (intent + context + inputs). DSPy-backed optimization is a Phase 3 integration for teams with labeled data.

## v1 Dependencies

```toml
dependencies = ["litellm>=1.0"]
requires-python = ">=3.11"

[project.optional-dependencies]
pydantic = ["pydantic>=2.0"]
all      = ["stratum[pydantic]"]
```

jsonschema and pyyaml are MCP-server-only (Phase 2) — not library dependencies.

## Phase 3 Integrations (not v1)

Build from observed pain, not schedule: Temporal (durable execution), Ray (distributed agents), Outlines (self-hosted constrained decoding via LiteLLM → vLLM → Outlines), DSPy (prompt optimization).

## TypeScript Library (Phase 2)

Zod for contracts, `@anthropic-ai/sdk` for LLM calls. Vercel AI SDK is an integration target for Next.js users — not the substrate.

## The IR

`.stratum.yaml` is generated by the library and by Claude (via MCP). Developers never write it. It is LLVM IR — the compiler targets it, nobody writes it from scratch. IR parsing/validation (jsonschema + pyyaml) is a Phase 2 concern, needed for the MCP server.
